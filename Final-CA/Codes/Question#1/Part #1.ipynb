{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, csv, os, io, string, pickle, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urllib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy.random import randn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=df_train.to_numpy()\n",
    "data_test=df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RATIO = 0.15\n",
    "VALIDATION_RATIO = 0.1\n",
    "MAX_DIM = 1\n",
    "MAX_LENGTH = 28\n",
    "NUM_OF_CLASSES = 10\n",
    "LR = 0.001\n",
    "LR_DECAY = 0.95\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "NUM_OF_EPOCHS = 4\n",
    "BATCH_SIZE = 100\n",
    "SAVE_PATH = 'params.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[:,1:]\n",
    "y_train = data_train[:,0]\n",
    "X_test =  data_test[:,1:]\n",
    "y_test =  data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_builder(name, alpha=1, bias=0):\n",
    "  def func(X):\n",
    "    if name == 'Relu':\n",
    "      X[X<=0] = 0\n",
    "    elif name == 'LeakyRelu':\n",
    "      X = np.max(alpha * X, X)\n",
    "    elif name == 'ELU':\n",
    "      X = alpha*(np.exp(X[X<=0])-1)\n",
    "    elif name == 'Tanh':\n",
    "      X = np.tanh(X)\n",
    "    elif name == 'Linear':\n",
    "      X = alpha*X + bias\n",
    "    return X\n",
    "  return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeFilter(size, scale = 1.0):\n",
    "    stddev = scale/np.sqrt(np.prod(size))\n",
    "    return np.random.normal(loc = 0, scale = stddev, size = size)\n",
    "\n",
    "def initializeWeight(size):\n",
    "    return np.random.standard_normal(size=size) * 0.01\n",
    "\n",
    "def initializeParams ():\n",
    "  m = [None]*len(Network)\n",
    "  v = [None]*len(Network)\n",
    "  mb = [None]*len(Network)\n",
    "  vb = [None]*len(Network)\n",
    "\n",
    "  for i, layer in enumerate(Network):\n",
    "    if layer[0] == 'maxpool' or layer[0] == 'flatten':\n",
    "      continue\n",
    "    m[i] = np.zeros(layer[1].shape)\n",
    "    v[i] = np.zeros(layer[1].shape)\n",
    "    mb[i] = np.zeros(layer[2].shape)\n",
    "    vb[i] = np.zeros(layer[2].shape)\n",
    "\n",
    "  return [m, v, mb, vb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoricalCrossEntropy(probs, label):\n",
    "    return -np.sum(label * np.log(probs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanargmax(arr):\n",
    "    idx = np.nanargmax(arr)\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy (X_test,Y_true):\n",
    "  \n",
    "  Y_pred = np.empty((np.shape(Y_true)))\n",
    "  Y_probs = np.empty((np.shape(Y_true)))\n",
    "  for i in range(len(X_test)):\n",
    "    Y_pred[i], Y_probs[i] = predict(X_test[i])\n",
    "  Y_pred = np.array(Y_pred).astype(int)\n",
    "  return np.sum((Y_pred==Y_true).astype(int))/len(Y_true)*100, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(data, weights, bias, stride):\n",
    "    filter_x, filter_y, filter_depth, filter_num = np.shape(weights)\n",
    "    data_x, data_y, data_depth = np.shape(data)\n",
    "    result = np.zeros((data_x, data_y, filter_num))\n",
    "    pad_size_x = filter_x // 2\n",
    "    pad_size_y = filter_y // 2\n",
    "    padded_data = np.pad(data, ((pad_size_x, pad_size_x), (pad_size_y, pad_size_y), (0, 0)), 'constant', constant_values=0)\n",
    "    for n in range(filter_num):\n",
    "        kernel_dy = pad_size_y\n",
    "        result_y = 0\n",
    "        while kernel_dy < data_y:\n",
    "            kernel_dx = pad_size_x\n",
    "            result_x  = 0\n",
    "            while kernel_dx < data_x:\n",
    "                result[result_x, result_y, n] = np.sum(np.multiply(weights[:, :, :, n], padded_data[kernel_dx:kernel_dx+filter_x, kernel_dy:kernel_dy+filter_y, :])) + bias[n]\n",
    "                kernel_dx += stride\n",
    "                result_x += 1\n",
    "            kernel_dy += stride\n",
    "            result_y += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(data, pool_f): #pool_f is tuple (2, 1), pool_stride\n",
    "    (data_x, data_y, data_depth) = np.shape(data)\n",
    "    dim_x = math.ceil(data_x / pool_f[0])\n",
    "    dim_y = math.ceil(data_y / pool_f[1])\n",
    "    downsampled = np.zeros((dim_x, dim_y, data_depth))\n",
    "    pad_size_x = pool_f[0] - data_x % pool_f[0]\n",
    "    pad_size_y = pool_f[1] - data_y % pool_f[1]\n",
    "    padded_data = np.pad(data, ((0, pad_size_x), (0, pad_size_y), (0, 0)), 'constant', constant_values=0)\n",
    "    for i in range(data_depth):\n",
    "        kernel_dy = 0\n",
    "        result_y = 0\n",
    "        while kernel_dy + pool_f[1] < np.shape(padded_data)[1]:\n",
    "            kernel_dx   = 0\n",
    "            result_x = 0\n",
    "            while kernel_dx + pool_f[0] < np.shape(padded_data)[0]:\n",
    "                downsampled[result_x, result_y, i] = np.max(padded_data[kernel_dx:kernel_dx+pool_f[0], kernel_dy:kernel_dy+pool_f[1], i])\n",
    "                kernel_dx += pool_f[0]\n",
    "                result_x += 1\n",
    "            kernel_dy += pool_f[1]\n",
    "            result_y += 1\n",
    "    return downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "   \n",
    "    (f_dim_x, f_dim_y, f_depth, num_f) = filt.shape\n",
    "    (orig_dim_x,orig_dim_y , _) = conv_in.shape\n",
    "   \n",
    "    dout = np.zeros(conv_in.shape) \n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((num_f,1))\n",
    "\n",
    "    for curr_f in range(num_f):\n",
    "      \n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f_dim_y <= orig_dim_y:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f_dim_x <= orig_dim_x:\n",
    "                dfilt[: , : , : ,curr_f] += dconv_prev[out_x, out_y,curr_f] * conv_in[curr_x:curr_x + f_dim_x, curr_y:curr_y + f_dim_y,:]  ###########################         \n",
    "                dout[curr_x:curr_x + f_dim_x, curr_y:curr_y + f_dim_y,:] += dconv_prev[out_x, out_y,curr_f] * filt[: , : , : ,curr_f] \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "       \n",
    "        dbias[curr_f] = np.sum(dconv_prev[: , : ,curr_f])\n",
    "    \n",
    "    return dout, dfilt, dbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpoolBackward(dpool, orig, pool_f):\n",
    "   \n",
    "    (orig_dim_x, orig_dim_y, orig_depth) = orig.shape\n",
    "    \n",
    "    dout = np.zeros(orig.shape)\n",
    "    \n",
    "    for curr_c in range(orig_depth):\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + pool_f[1] <= orig_dim_y:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + pool_f[0] <= orig_dim_x:\n",
    "                \n",
    "                (a, b) = nanargmax(orig[curr_x:curr_x+ pool_f[0] , curr_y:curr_y + pool_f[1], curr_c])\n",
    "                dout[ curr_x+a, curr_y+b, curr_c] = dpool[out_x, out_y, curr_c]\n",
    "                \n",
    "                curr_x += pool_f[0]\n",
    "                out_x += 1\n",
    "            curr_y += pool_f[1]\n",
    "            out_y += 1\n",
    "        \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(raw_preds):\n",
    "    out = np.exp(raw_preds) \n",
    "    return out/np.sum(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv, maxpool, fullyconn, flatten, softmax\n",
    "Network = []\n",
    "Network.append(['conv', initializeFilter((5, 5, 1, 8)), np.zeros((8,1)), 'Relu']) ##########\n",
    "Network.append(['conv', initializeFilter((5, 5, 8, 8)), np.zeros((8,1)), 'Relu'])\n",
    "Network.append(['maxpool', (2, 2)])\n",
    "\n",
    "\n",
    "#Network.append(['conv', initializeFilter((5, 5, 8, 16)), np.zeros((16,1)), 'Relu']) ##########\n",
    "#Network.append(['conv', initializeFilter((5, 5, 16, 16)), np.zeros((16,1)), 'Relu'])\n",
    "#Network.append(['maxpool', (2, 2)])\n",
    "\n",
    "#Network.append(['conv', initializeFilter((5, 5, 16, 32)), np.zeros((32,1)), 'Relu']) ##########\n",
    "#Network.append(['conv', initializeFilter((5, 5, 32, 32)), np.zeros((32,1)), 'Relu'])\n",
    "#Network.append(['maxpool', (2, 2)])\n",
    "\n",
    "#Network.append(['conv', initializeFilter((3, 1, 32, 32)), np.zeros((32,1)), 'Relu'])\n",
    "Network.append(['flatten',])\n",
    "\n",
    "Network.append(['fullyconn', initializeWeight((200, 1568)), np.zeros((200,1)), 'Relu'])\n",
    "Network.append(['fullyconn', initializeWeight((10, 200)), np.zeros((10,1)), 'Softmax'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim_x = MAX_LENGTH\n",
    "data_dim_y = 1\n",
    "data_depth = MAX_DIM\n",
    "f_dim1 = 3\n",
    "f_dim2 = 1\n",
    "num_filt1 = 64\n",
    "num_filt2 = 64\n",
    "num_neurons = 100\n",
    "flatten_size = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_forward (data):\n",
    "    results = [None]*(len(Network)+1)\n",
    "    results[0] = data\n",
    "    for i, layer in enumerate(Network):\n",
    "      if (layer[0] == 'conv'):\n",
    "        results[i+1] = convolution(results[i], layer[1], layer[2], 1)\n",
    "        results[i+1] = function_builder(layer[3])(results[i+1])\n",
    "      elif (layer[0] == 'maxpool'):\n",
    "        results[i+1] = maxpool(results[i], layer[1])\n",
    "      elif (layer[0] == 'flatten'):\n",
    "        dim1, dim2, dim3 = np.shape(results[i])\n",
    "        results[i+1] = results[i].reshape((dim1*dim2*dim3, 1))\n",
    "      elif (layer[0] == 'fullyconn'):\n",
    "        results[i+1] = layer[1].dot(results[i]) + layer[2]\n",
    "        if layer[3] == 'Softmax':\n",
    "          results[i+1] = softmax(results[i+1])\n",
    "          break\n",
    "        results[i+1] = function_builder(layer[3])(results[i+1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_backward (results, label):\n",
    "    dNetwork = [[None, None]]*len(Network)\n",
    "    dresults = [None]*(len(Network)+1)\n",
    "    dresults[-1] = results[-1] - label\n",
    "    for i, layer in reversed(list(enumerate(Network))):\n",
    "      if (layer[0] == 'conv'):\n",
    "        dresults[i], df, db = convolutionBackward(dresults[i+1], results[i], layer[1], 1)\n",
    "        dNetwork[i] = [df, db]\n",
    "        if i != 0 and Network[i-1][0] == 'conv':\n",
    "          dresults[i][results[i]<=0] = 0\n",
    "      elif (layer[0] == 'maxpool'):\n",
    "        dresults[i] = maxpoolBackward(dresults[i+1], results[i], layer[1])\n",
    "        dresults[i][results[i]<=0] = 0\n",
    "      elif (layer[0] == 'flatten'):\n",
    "        dresults[i] = dresults[i+1].reshape(np.shape(results[i]))\n",
    "      elif (layer[0] == 'fullyconn'):\n",
    "        dw = dresults[i+1].dot(results[i].T)\n",
    "        db = np.sum(dresults[i+1], axis=1).reshape(layer[2].shape)\n",
    "        dNetwork[i] = [dw, db]\n",
    "        dresults[i] = layer[1].T.dot(dresults[i+1])\n",
    "        if Network[i-1][0] == 'flatten':\n",
    "          continue\n",
    "        dresults[i][results[i]<=0] = 0\n",
    "    return dNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN (data, label):\n",
    "    \n",
    "    results = CNN_forward (data)\n",
    "    dNetwork = CNN_backward(results, label)\n",
    "\n",
    "    loss = categoricalCrossEntropy(results[-1], label)\n",
    "    grads = dNetwork\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentumGD(dNetwork, lr, params):\n",
    "    [m, v, mb, vb] = params\n",
    "    for i, layer in enumerate(Network):\n",
    "\n",
    "      if layer[0] == 'maxpool' or layer[0] == 'flatten':\n",
    "        continue\n",
    "      v[i] = GAMMA*v[i] + lr*(dNetwork[i][0]/BATCH_SIZE)\n",
    "      layer[1] -= v[i]\n",
    "\n",
    "      vb[i] = GAMMA*vb[i] + lr*(dNetwork[i][1]/BATCH_SIZE)\n",
    "      layer[2] -= vb[i]\n",
    "\n",
    "    return [m, v, mb, vb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamGD (dNetwork, lr, params):\n",
    "    [m, v, mb, vb] = params\n",
    "    for i, layer in enumerate(Network):\n",
    "\n",
    "      if layer[0] == 'maxpool' or layer[0] == 'flatten':\n",
    "        continue\n",
    "      m[i] = BETA1*m[i] + (1-BETA1)*dNetwork[i][0]/BATCH_SIZE \n",
    "      v[i] = BETA2*v[i] + (1-BETA2)*(dNetwork[i][0]/BATCH_SIZE)**2 \n",
    "      m_hat = m[i]/(1-BETA1)\n",
    "      v_hat = v[i]/(1-BETA2)\n",
    "      layer[1] -= lr * m_hat/(np.sqrt(v_hat)+1e-7)\n",
    "\n",
    "      mb[i] = BETA1*mb[i] + (1-BETA1)*dNetwork[i][1]/BATCH_SIZE\n",
    "      vb[i] = BETA2*vb[i] + (1-BETA2)*(dNetwork[i][1]/BATCH_SIZE)**2\n",
    "      mb_hat = mb[i]/(1-BETA1)\n",
    "      vb_hat = vb[i]/(1-BETA2)\n",
    "      layer[2] -= lr * mb_hat/(np.sqrt(vb_hat)+1e-7)\n",
    "\n",
    "    return [m, v, mb, vb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver(batch, lr, params, cost, optimizer='ADAM'):\n",
    "    \n",
    "    X = batch[:,0:-1] \n",
    "    Y = batch[:,-1] \n",
    "\n",
    "    X = X.reshape(len(batch),MAX_LENGTH,MAX_LENGTH,MAX_DIM)   \n",
    "   \n",
    "    cost_ = 0\n",
    "    \n",
    "    dNetwork = [[0, 0]]*len(Network)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        \n",
    "        x = X[i]\n",
    "        y = np.eye(NUM_OF_CLASSES)[int(Y[i])].reshape(NUM_OF_CLASSES, 1) \n",
    "\n",
    "        grads, loss = CNN(x, y)\n",
    "        for j, grad in enumerate(grads):\n",
    "          if grad[0] is None:\n",
    "            continue\n",
    "          if i == 0:\n",
    "            dNetwork[j] = [np.zeros(grad[0].shape), np.zeros(grad[1].shape)]\n",
    "          dNetwork[j][0] += grad[0]\n",
    "          dNetwork[j][1] += grad[1]\n",
    "        cost_+= loss\n",
    "\n",
    "    if optimizer == 'ADAM':\n",
    "      new_params = adamGD(dNetwork, lr, params)\n",
    "    elif optimizer == 'MOMENTUM':\n",
    "      new_params = momentumGD(dNetwork, lr, params)\n",
    "\n",
    "    cost_ = cost_/BATCH_SIZE\n",
    "    cost.append(cost_)\n",
    "\n",
    "    return new_params, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    results = CNN_forward(data)\n",
    "    return np.argmax(results[-1]), np.max(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.001, Batch Size:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cost: 1.980024: 100%|██████████████████████████████████████████████████████████████████| 10/10 [05:01<00:00, 30.19s/it]\n",
      "Cost: 0.461699: 100%|██████████████████████████████████████████████████████████████████| 10/10 [04:34<00:00, 27.49s/it]\n",
      "Cost: 0.652358: 100%|██████████████████████████████████████████████████████████████████| 10/10 [05:02<00:00, 30.26s/it]\n",
      "Cost: 0.458555: 100%|██████████████████████████████████████████████████████████████████| 10/10 [05:06<00:00, 30.68s/it]\n"
     ]
    }
   ],
   "source": [
    "params = initializeParams()\n",
    "\n",
    "m =5000\n",
    "X = X_train[0:m,:]\n",
    "len(X)\n",
    "y_dash = y_train[0:m].reshape(m,1)\n",
    "mean=int(np.mean(X))\n",
    "std= int(np.std(X))\n",
    "X=(X-mean)/std\n",
    "trainData = np.hstack((X,y_dash))\n",
    "cost = []\n",
    "print(\"LR:\"+str(LR)+\", Batch Size:\"+str(BATCH_SIZE))\n",
    "for epoch in range(NUM_OF_EPOCHS):\n",
    "    np.random.shuffle(trainData)\n",
    "    batches = np.array([trainData[k:k + BATCH_SIZE] for k in range(0, trainData.shape[0], BATCH_SIZE)])\n",
    "    t = tqdm(batches)\n",
    "    for x,batch in enumerate(t):\n",
    "      \n",
    "        params, cost = solver(batch, LR*(LR_DECAY**x), params, cost, 'ADAM')\n",
    "        t.set_description(\"Cost: %.6f\" % (cost[-1]))\n",
    "with open(SAVE_PATH, 'wb') as file:\n",
    "    pickle.dump(Network, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Parameters from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = pickle.load(open('pramas.plk', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is equal to: 89.0\n"
     ]
    }
   ],
   "source": [
    "m =1000\n",
    "X = X_train[0:m,:]\n",
    "len(X)\n",
    "y_dash = y_train[0:m].reshape(m,1)\n",
    "mean=int(np.mean(X))\n",
    "std= int(np.std(X))\n",
    "#################################################\n",
    "m =200\n",
    "X = X_test[0:m,:]\n",
    "y_dash = y_test[0:m]\n",
    "X=(X-mean)/std\n",
    "X = X.reshape(len(X),28,28,1)\n",
    "acc, Y_pred = calc_accuracy(X,y_dash)\n",
    "print(\"accuracy is equal to:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_backward (results, label):\n",
    "    dNetwork = [[None, None]]*len(Network)\n",
    "    dresults = [None]*(len(Network)+1)\n",
    "    dresults[-1] = results[-1] - label\n",
    "    for i, layer in reversed(list(enumerate(Network))):\n",
    "      if (layer[0] == 'conv'):\n",
    "        dresults[i], df, db = convolutionBackward(dresults[i+1], results[i], layer[1], 1)\n",
    "        dNetwork[i] = [df, db]\n",
    "        if i != 0 and Network[i-1][0] == 'conv':\n",
    "          dresults[i][results[i]<=0] = 0\n",
    "      elif (layer[0] == 'maxpool'):\n",
    "        dresults[i] = maxpoolBackward(dresults[i+1], results[i], layer[1])\n",
    "        dresults[i][results[i]<=0] = 0\n",
    "      elif (layer[0] == 'flatten'):\n",
    "        dresults[i] = dresults[i+1].reshape(np.shape(results[i]))\n",
    "      elif (layer[0] == 'fullyconn'):\n",
    "        dw = dresults[i+1].dot(results[i].T)\n",
    "        db = np.sum(dresults[i+1], axis=1).reshape(layer[2].shape)\n",
    "        dNetwork[i] = [dw, db]\n",
    "        dresults[i] = layer[1].T.dot(dresults[i+1])\n",
    "        if Network[i-1][0] == 'flatten':\n",
    "          continue\n",
    "        dresults[i][results[i]<=0] = 0\n",
    "    return dNetwork,dresults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN (data, label):\n",
    "    \n",
    "    results = CNN_forward (data)\n",
    "    dNetwork,dresults = CNN_backward(results, label)\n",
    "\n",
    "    loss = categoricalCrossEntropy(results[-1], label)\n",
    "    grads = dNetwork\n",
    "\n",
    "    return dresults, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flat_points(latent_dim, n_samples):\n",
    "    \n",
    "    x_input=255 * np.random.rand(28, 28, 1)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMAGE_GENERATOR(label,beta1,beta2,lr):\n",
    "    our_noise=255 * np.random.rand(28, 28, 1)\n",
    "\n",
    "    image=np.zeros((28, 28, 1))\n",
    "\n",
    "    image=image-mean\n",
    "    image=image/std\n",
    "\n",
    "    v1 = np.zeros(image.shape)\n",
    "    s1 = np.zeros(image.shape)\n",
    "\n",
    "    for i in range(200):\n",
    "    \n",
    "        y = np.eye(10)[int(label)].reshape(10, 1) \n",
    "        dresults,loss=CNN(image,y)\n",
    "        print(dresults.shape)\n",
    "        v1 = beta1*v1 + (1-beta1)*dresults\n",
    "        s1 = beta2*s1 + (1-beta2)*(dresults)**2 \n",
    "        v1_hat = v1/(1-beta1)\n",
    "        s1_hat = s1/(1-beta2)\n",
    "        image-= lr * v1_hat/(np.sqrt(s1_hat)+1e-7)\n",
    "        print(loss)\n",
    "    \n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "lr = 0.001\n",
    "label=3\n",
    "IMAGE_GENERATOR(label,beta1,beta2,lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
